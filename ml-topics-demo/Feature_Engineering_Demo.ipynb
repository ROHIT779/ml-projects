{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "djg4ch89Mr3e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from category_encoders import TargetEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target Encoding - It transformes categorical variables to numerical values.\n",
        "For each category in categorical feature, it calculates mean (or, any other aggregate statistics like median or sum) of the target variable and replaces each category with its target mean.  \n",
        "Pros:  \n",
        "1. Dimensionality Reduction: Unlike one-hot-encoding it does not create new\n",
        "columns for each categories, which is beneficial for high-cardinality categorical features.  \n",
        "\n",
        "Cons:  \n",
        "1. Overfitting: For low-frequency categories, the encoded value might be\n",
        "heavily influenced by a small number of samples, causing overfitting to those categories. Techniques like smoothing can help here. In smoothing, instead of taking mean of the target variable, we will calculate weighted mean for each categories, the weights being the frequencies of the categories.  \n",
        "2. Target Leakage: It is important to apply the target mapping to only training data and learn the mapping and apply it on the test data. Otherwise if we apply the mapping on the full dataset, the training data will contain test data target variable results, may result in a model which performs overwhelmingly on test set."
      ],
      "metadata": {
        "id": "SeuWUkz6R-GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# target encoding\n",
        "data = {\n",
        "    'category': ['A', 'B', 'A', 'C', 'C', 'B', 'A', 'A', 'C'],\n",
        "    'target': [12, 21, 14, 35, 29, 18, 11, 15, 32]\n",
        "}\n",
        "data = pd.DataFrame(data)\n",
        "encoder = TargetEncoder(cols = ['category'])\n",
        "data_encoded = encoder.fit_transform(data['category'], data['target'])\n",
        "print(data_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqEYJePDQwfS",
        "outputId": "732edd02-3b2f-4cc3-f425-36ed6fb827f2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    category\n",
            "0  19.471254\n",
            "1  20.596524\n",
            "2  19.471254\n",
            "3  22.511221\n",
            "4  22.511221\n",
            "5  20.596524\n",
            "6  19.471254\n",
            "7  19.471254\n",
            "8  22.511221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polynomial Features - Linear models might fail to capture non-linear relationships in the data. Using polynomial features allows a linear model to learn non-linear relationships.  \n",
        "When to use:\n",
        "1. When a linear model underfits data.  \n",
        "2. When the relationship between input and output is non-linear.  \n",
        "3. When we want to capture interaction effects between features.  \n",
        "\n",
        "Considerations:  \n",
        "1. Higher degree polynomials may introduce too many features resulting in overfitting. Regularization (Lasso, Ridge, ElasticNet) helps control this by penalizing large coefficients.  \n",
        "2. Polynomial features often introduce highly correlated variables. Using PCA can help in reducing redundancy.  \n",
        "\n",
        "When not to use:  \n",
        "1. When there is high dimensional data, it will lead to too many features resulting in feature explosion and computation inefficiency.  \n",
        "2. Nosiy Data: High degree polynomials are sensitive to noisy data, resulting in overfitting.  \n",
        "3. Better alternatives: Decision Trees, SVMs and Neural Networks can model non-linearity better."
      ],
      "metadata": {
        "id": "0Dmoz-ifhYtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# polynomial features\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "X = np.array([[2], [3], [4]])\n",
        "poly = PolynomialFeatures(degree = 2, include_bias = False)\n",
        "X_poly = poly.fit_transform(X)\n",
        "print(f\"Polynomial Features: \\n{X_poly}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCBY_orniuSk",
        "outputId": "ea3528fe-0e78-4dcc-8554-b174d9653bd6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polynomial Features: \n",
            "[[ 2.  4.]\n",
            " [ 3.  9.]\n",
            " [ 4. 16.]]\n"
          ]
        }
      ]
    }
  ]
}